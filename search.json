[
  {
    "objectID": "stablediffusionpipline.html",
    "href": "stablediffusionpipline.html",
    "title": "Exploring Stable diffusion Pipeline",
    "section": "",
    "text": "Load necessary modules\nSetup the notebookâ€™s configuration parameters\n\nparams=Parameters().from_json('../config.json')\n\n\nparams.gpu = device_by_name(\"Tesla\")\nparams.height = 512  # default height of Stable Diffusion\nparams.width = 512  # default width of Stable Diffusion\nparams.num_inference_steps = 25  # Number of denoising steps\nparams.guidance_scale = 7.5  # Scale for classifier-free guidance\nparams.seed = 0\n\nwe will use the gpu\n\ndevice=params.gpu\n\nload pretrained model\n\nparams.model_name='stabilityai/stable-diffusion-2-1-base'\n\n\npipe = StableDiffusionPipeline.from_pretrained(params.model_name, torch_dtype=torch.float16 , requires_safety_checker = False).to(device)\n# pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n# # Workaround for not accepting attention shape using VAE for Flash Attention\n# pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n\n\n\n\nSetup prompt\n\nprompt = [\"a photograph of a cute puppy\"]\ngenerator = torch.Generator(device = device)\ngenerator.manual_seed(params.seed)  # Seed generator to create the inital latent noise\n\n<torch._C.Generator>\n\n\nRun the standard model\n\nimage = pipe(prompt, generator = generator).images[0]\n\nimage\n\n\n\n\n\n\n\nlets try another prompt\n\nprompt = [\"a graffity of the monalisa on a brick wall, a red gorilla painter\"]\n\nimage = pipe(prompt, generator = generator).images[0]\n\nimage\n\n\n\n\n\n\n\nnegative_prompt\n\nprompt = [\"a red gorilla painter painting a graffity of the monalisa on a brick wall\"]\nnegative_prompt = [\"a photo\"]\n\nimage = pipe(prompt, negative_prompt = negative_prompt, generator = generator).images[0]\n\nimage"
  },
  {
    "objectID": "traindiffusers.html",
    "href": "traindiffusers.html",
    "title": "Training a Diffuser",
    "section": "",
    "text": "Setup basic parameters\nSince the model checkpoints are quite large, install Git-LFS to version these large files:\nSet training hyperparameters"
  },
  {
    "objectID": "traindiffusers.html#load-the-dataset",
    "href": "traindiffusers.html#load-the-dataset",
    "title": "Training a Diffuser",
    "section": "Load the dataset",
    "text": "Load the dataset\nLoad the Smithsonian Butterflies dataset with the ðŸ¤— Datasets library:\n\nparams.config.dataset_name = \"huggan/smithsonian_butterflies_subset\"\ndataset = load_dataset(params.config.dataset_name, split=\"train\")\n\nLetâ€™s explore the dataset and look at some images\n\nprint(f'The dataset has {len(dataset)} images')\nfig, axs = plt.subplots(1, 4, figsize=(16, 4))\nfor i, image in enumerate(dataset[:4][\"image\"]):\n    _ = axs[i].imshow(image)\n    axs[i].set_axis_off()\nfig.show()\n\nThe dataset has 1000 images\n\n\n\n\n\nWe want to create a transform that would resize the image to the same size, do random flip and create a tensor\n\npreprocess = transforms.Compose(\n    [\n        transforms.Resize((params.config.image_size, params.config.image_size)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5]),\n    ]\n)\n\ndef transform(examples):\n    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n    return {\"images\": images}\n\n\ndataset.set_transform(transform)\n\nDefine dataloader\n\ntrain_dataloader = torch.utils.data.DataLoader(dataset, batch_size=params.config.train_batch_size, shuffle=True)"
  },
  {
    "objectID": "traindiffusers.html#create-a-unet2dmodel",
    "href": "traindiffusers.html#create-a-unet2dmodel",
    "title": "Training a Diffuser",
    "section": "Create a UNet2DModel",
    "text": "Create a UNet2DModel\n\nmodel = UNet2DModel(\n    sample_size=params.config.image_size,  # the target image resolution\n    in_channels=3,  # the number of input channels, 3 for RGB images\n    out_channels=3,  # the number of output channels\n    layers_per_block=2,  # how many ResNet layers to use per UNet block\n    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n    down_block_types=(\n        \"DownBlock2D\",  # a regular ResNet downsampling block\n        \"DownBlock2D\",\n        \"DownBlock2D\",\n        \"DownBlock2D\",\n        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n        \"DownBlock2D\",\n    ),\n    up_block_types=(\n        \"UpBlock2D\",  # a regular ResNet upsampling block\n        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n        \"UpBlock2D\",\n        \"UpBlock2D\",\n        \"UpBlock2D\",\n        \"UpBlock2D\",\n    ),\n)\n\nChecking the model sample outputâ€™s shape matches the inputâ€™s shape\n\nsample_image = dataset[0][\"images\"].unsqueeze(0)\nprint(\"Input shape:\", sample_image.shape)\n\nprint(\"Output shape:\", model(sample_image, timestep=0).sample.shape)\n\nInput shape: torch.Size([1, 3, 128, 128])\nOutput shape: torch.Size([1, 3, 128, 128])"
  },
  {
    "objectID": "traindiffusers.html#create-a-scheduler",
    "href": "traindiffusers.html#create-a-scheduler",
    "title": "Training a Diffuser",
    "section": "Create a scheduler",
    "text": "Create a scheduler\n\nnoise_scheduler = DDPMScheduler(num_train_timesteps=1000)\nnoise = torch.randn(sample_image.shape)\ntimesteps = torch.LongTensor([50])\nnoisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n\nImage.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
  },
  {
    "objectID": "traindiffusers.html#define-loss-and-prediction",
    "href": "traindiffusers.html#define-loss-and-prediction",
    "title": "Training a Diffuser",
    "section": "Define loss and prediction",
    "text": "Define loss and prediction\n\nnoise_pred = model(noisy_image, timesteps).sample\nloss = F.mse_loss(noise_pred, noise)"
  },
  {
    "objectID": "traindiffusers.html#train-the-model",
    "href": "traindiffusers.html#train-the-model",
    "title": "Training a Diffuser",
    "section": "Train the model",
    "text": "Train the model\noptimizer and a learning rate scheduler\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=params.config.learning_rate)\nlr_scheduler = get_cosine_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=params.config.lr_warmup_steps,\n    num_training_steps=(len(train_dataloader) * params.config.num_epochs),\n)\n\nEvaluation functions\n\ndef make_grid(images, rows, cols):\n    w, h = images[0].size\n    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n    for i, image in enumerate(images):\n        grid.paste(image, box=(i % cols * w, i // cols * h))\n    return grid\n\n\ndef evaluate(config, epoch, pipeline):\n    # Sample some images from random noise (this is the backward diffusion process).\n    # The default pipeline output type is `List[PIL.Image]`\n    images = pipeline(\n        batch_size=params.config.eval_batch_size,\n        generator=torch.manual_seed(config.seed),\n    ).images\n\n    # Make a grid out of the images\n    image_grid = make_grid(images, rows=4, cols=4)\n\n    # Save the images\n    test_dir = os.path.join(config.output_dir, f\"samples\")\n    os.makedirs(test_dir, exist_ok=True)\n    image_grid.save(f\"{test_dir}/{epoch:04d}_{config.seed}.png\")\n\n\nTraining Loop\n\ndef get_full_repo_name(model_id: str, organization: str = None, token: str = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"\n\n\ndef train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n    # Initialize accelerator and tensorboard logging\n    accelerator = Accelerator(\n        mixed_precision=config.mixed_precision,\n        gradient_accumulation_steps=config.gradient_accumulation_steps,\n        log_with=\"tensorboard\",\n        logging_dir=os.path.join(config.output_dir, \"logs\"),\n    )\n    if accelerator.is_main_process:\n        if config.push_to_hub:\n            repo_name = get_full_repo_name(Path(config.output_dir).name)\n            repo = Repository(config.output_dir, clone_from=repo_name)\n        elif config.output_dir is not None:\n            os.makedirs(config.output_dir, exist_ok=True)\n        accelerator.init_trackers(\"train_example\")\n\n    # Prepare everything\n    # There is no specific order to remember, you just need to unpack the\n    # objects in the same order you gave them to the prepare method.\n    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, lr_scheduler\n    )\n\n    global_step = 0\n\n    # Now you train the model\n    for epoch in range(config.num_epochs):\n        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n        progress_bar.set_description(f\"Epoch {epoch}\")\n\n        for step, batch in enumerate(train_dataloader):\n            clean_images = batch[\"images\"]\n            # Sample noise to add to the images\n            noise = torch.randn(clean_images.shape).to(clean_images.device)\n            bs = clean_images.shape[0]\n\n            # Sample a random timestep for each image\n            timesteps = torch.randint(\n                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device\n            ).long()\n\n            # Add noise to the clean images according to the noise magnitude at each timestep\n            # (this is the forward diffusion process)\n            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n\n            with accelerator.accumulate(model):\n                # Predict the noise residual\n                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n                loss = F.mse_loss(noise_pred, noise)\n                accelerator.backward(loss)\n\n                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            progress_bar.update(1)\n            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n            progress_bar.set_postfix(**logs)\n            accelerator.log(logs, step=global_step)\n            global_step += 1\n\n        # After each epoch you optionally sample some demo images with evaluate() and save the model\n        if accelerator.is_main_process:\n            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n\n            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n                evaluate(config, epoch, pipeline)\n\n            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n                if config.push_to_hub:\n                    repo.push_to_hub(commit_message=f\"Epoch {epoch}\", blocking=True)\n                else:\n                    pipeline.save_pretrained(config.output_dir)\n\nLetâ€™s train\n\nargs = (params.config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n\nnotebook_launcher(train_loop, args, num_processes=1)\n\nLaunching training on one GPU.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView the output\n\nsample_images = sorted(glob.glob(f\"{params.config.output_dir}/samples/*.png\"))\nImage.open(sample_images[-1])\n\n\n\n\n\nsample_images = sorted(glob.glob(f\"{params.config.output_dir}/samples/*.png\"))\nImage.open(sample_images[-2])\n\n\n\n\n\nImage.open(sample_images[-3])"
  },
  {
    "objectID": "diffuserspipeline.html",
    "href": "diffuserspipeline.html",
    "title": "Building Diffuser Pipeline",
    "section": "",
    "text": "Load necessary modules\nSetup the notebookâ€™s configuration parameters\n\nparams=Parameters().from_json('../config.json')\n\n\nparams.gpu = device_by_name(\"Tesla\")\nparams.height = 512  # default height of Stable Diffusion\nparams.width = 512  # default width of Stable Diffusion\nparams.num_inference_steps = 25  # Number of denoising steps\nparams.guidance_scale = 7.5  # Scale for classifier-free guidance\nparams.seed = 4356\n\nwe will use the gpu\n\ndevice=params.gpu\n\nload and define the diffusersâ€™s building blocks\n\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder = \"vae\").to(device)\ntokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder = \"tokenizer\")\ntext_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder = \"text_encoder\").to(device)\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder = \"unet\").to(device)\n\nDefine the scheduler\n\nscheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n\nSetup prompt\n\nprompt = [\"a photograph of a cute puppy\"]\ngenerator = torch.manual_seed(params.seed)  # Seed generator to create the inital latent noise\nbatch_size = len(prompt)\n\ncalculate the embeddings from the prompt\n\ntext_input = tokenizer(\n    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n)\n\nwith torch.no_grad():\n    text_embeddings = text_encoder(text_input.input_ids.to(device))[0]\n\ncalculate the embeddings for an empty prompt\n\nmax_length = text_input.input_ids.shape[-1]\nuncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\nuncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]\n\nThe modelâ€™s input is a concatenation of the promptâ€™s and the null promptâ€™s embeddings\n\ntext_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\nGenerate some initial random noise as a starting point for the diffusion process in the latent space\n\nlatents = torch.randn(\n    (batch_size, unet.in_channels, params.height // 8, params.width // 8),\n    generator=generator,\n)\nlatents = latents.to(device)\n\nStart by scaling the input with the initial noise distribution, sigma, the noise scale value\n\nlatents = latents * scheduler.init_noise_sigma\n\nHere is the denoising loop where the magic is done\n\nscheduler.set_timesteps(params.num_inference_steps)\n\nfor t in tqdm(scheduler.timesteps):\n    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n    latent_model_input = torch.cat([latents] * 2)\n\n    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n\n    # predict the noise residual\n    with torch.no_grad():\n        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n    # perform guidance\n    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n    noise_pred = noise_pred_uncond + params.guidance_scale * (noise_pred_text - noise_pred_uncond)\n    # compute the previous noisy sample x_t -> x_t-1\n    latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n\n\n\nThe final step is to use the vae to decode the latent representation into an image\n\n# scale and decode the image latents with vae\nlatents = 1 / 0.18215 * latents\nwith torch.no_grad():\n    image = vae.decode(latents).sample\n\nLastly, convert the image to a PIL\n\nimage = (image / 2 + 0.5).clamp(0, 1)\nimage = image.detach().cpu().permute(0, 2, 3, 1).numpy()\nimages = (image * 255).round().astype(\"uint8\")\npil_images = [Image.fromarray(image) for image in images]\npil_images[0]"
  }
]